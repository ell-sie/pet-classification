{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Name: Dog Breed Classification using Convolutional Neural Networks\n",
    "\n",
    "Libraries Used:\n",
    "numpy and pandas for data manipulation\n",
    "matplotlib for data visualization\n",
    "tqdm for progress tracking\n",
    "keras for building and training neural networks\n",
    "sklearn for preprocessing data and evaluating models\n",
    "\n",
    "Dataset:\n",
    "The dataset used is the \"Dog Breed Identification\" dataset, consisting of images of various dog breeds.\n",
    "It contains a total of 10,222 images belonging to 120 dog breeds.\n",
    "For this project, a subset of five dog breeds was selected: Scottish Deerhound, Maltese Dog, Afghan Hound, Entlebucher, and Bernese Mountain Dog.\n",
    "\n",
    "Model Implementation:\n",
    "Baseline Model: A simple convolutional neural network (CNN) model was implemented without any optimization techniques.\n",
    "Optimized Model: An optimized CNN model was implemented using three regularization techniques: L1 regularization, L2 regularization, and a combination of both.\n",
    "\n",
    "Training and Evaluation:\n",
    "The dataset was split into training, validation, and test sets.\n",
    "Both models were trained using the training set and evaluated on the validation and test sets.\n",
    "Model performance metrics such as accuracy were recorded during training and evaluated after training completion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the labels data into dataframe and viewing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labels.csv file and check shape and records\n",
    "labels_all = pd.read_csv('./dogbreedidfromcomp/labels.csv')\n",
    "print(labels_all.shape)\n",
    "labels_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading number or each breed\n",
    "breed_all = labels_all['breed']\n",
    "breed_count = breed_all.value_counts()\n",
    "breed_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all breeds because i have high computation power\n",
    "CLASS_NAME = ['scottish_deerhound', 'maltese_dog', 'afghan_hound', 'entlebucher', 'bernese_mountain_dog']\n",
    "labels = labels_all[(labels_all['breed'].isin(CLASS_NAME))]\n",
    "labels = labels.reset_index()\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating numpy matrix with zeros\n",
    "X_data = np.zeros((len(labels), 224, 224, 3), dtype='float32')\n",
    "# One hot encoding\n",
    "Y_data = label_binarize(labels['breed'], classes = CLASS_NAME)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Reading and converting image to numpy array and normalizing dataset\n",
    "for i in tqdm(range(len(labels))):\n",
    "    img = image.load_img('./dogbreedidfromcomp/train/%s.jpg' % labels['id'][i], target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    x = np.expand_dims(img.copy(), axis=0)\n",
    "    X_data[i] = x / 255.0\n",
    "\n",
    "# Printing train image and one hot encode shape & size\n",
    "print('\\nTrain Images shape: ',X_data.shape,' size: {:,}'.format(X_data.size))\n",
    "print('One-hot encoded output shape: ',Y_data.shape,' size: {:,}'.format(Y_data.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we will create a network architecture for the model. We have used different types of layers according to their features namely Conv_2d (It is used to create a convolutional kernel that is convolved with the input layer to produce the output tensor), max_pooling2d (It is a downsampling technique which takes out the maximum value over the window defined by poolsize), flatten (It flattens the input and creates a 1D output), Dense (Dense layer produce the output as the dot product of input and kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5), activation ='relu', input_shape = (224,224,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (7,7), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = \"relu\", kernel_regularizer = 'l2'))\n",
    "model.add(Dense(64, activation = \"relu\", kernel_regularizer = 'l2'))\n",
    "model.add(Dense(len(CLASS_NAME), activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0001),metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the network architecture we found out the total parameters as 162,619."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After defining the network architecture we will start with splitting the test and train data then dividing train data in train and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data set into training and testing data sets\n",
    "X_train_and_val, X_test, Y_train_and_val, Y_test = train_test_split(X_data, Y_data, test_size = 0.1)\n",
    "# Splitting the training data set into training and validation data sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_and_val, Y_train_and_val, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epochs = 20\n",
    "batch_size = 84\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we analyse how the model is learning with each epoch in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['accuracy'], color='r')\n",
    "plt.plot(history.history['val_accuracy'], color='b')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use predict function to make predictions using this model also we are finding out the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Accuracy over the test set: \\n ', round((score[1]*100), 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting image to compare\n",
    "plt.imshow(X_test[1,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# Finding max value from predition list and comaparing original value vs predicted\n",
    "print(\"Originally : \",labels['breed'][np.argmax(Y_test[1])])\n",
    "print(\"Predicted : \",labels['breed'][np.argmax(Y_pred[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the first model\n",
    "model.save('saved_models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with downloading the dataset creating the model and finding out the predictions using the model. We can optimize different hyper parameters in order to tune this model for a higher accuracy. This model can be used to predict different breeds of dogs which can be further used by different NGO's working on saving animals and for educational purposes also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model with Regularization and Dropout\n",
    "model_optimized = Sequential()\n",
    "\n",
    "model_optimized.add(Conv2D(filters=64, kernel_size=(5,5), activation='relu', input_shape=(224,224,3), kernel_regularizer=l1(0.01)))\n",
    "model_optimized.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model_optimized.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model_optimized.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model_optimized.add(Conv2D(filters=16, kernel_size=(7,7), activation='relu', kernel_regularizer=l1(0.01)))\n",
    "model_optimized.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model_optimized.add(Conv2D(filters=8, kernel_size=(5,5), activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model_optimized.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model_optimized.add(Flatten())\n",
    "model_optimized.add(Dense(128, activation=\"relu\", kernel_regularizer=l1(0.01)))\n",
    "model_optimized.add(Dropout(0.5))  # Dropout layer\n",
    "model_optimized.add(Dense(64, activation=\"relu\", kernel_regularizer=l1(0.01)))\n",
    "model_optimized.add(Dense(len(CLASS_NAME), activation=\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model with the same loss function and optimizer\n",
    "model_optimized.compile(loss='categorical_crossentropy', optimizer=RMSprop(0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the same parameters as before\n",
    "history_optimized = model_optimized.fit(datagen.flow(X_train, Y_train, batch_size=84), epochs=20, validation_data=(X_val, Y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history for the optimized model\n",
    "plt.figure(figsize=(12,  5))\n",
    "plt.plot(history_optimized.history['accuracy'], color='r')\n",
    "plt.plot(history_optimized.history['val_accuracy'], color='b')\n",
    "plt.title('Optimized Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the optimized model\n",
    "Y_pred_optimized = model_optimized.predict(X_test)\n",
    "score_optimized = model_optimized.evaluate(X_test, Y_test)\n",
    "print('Accuracy over the test set for optimized model: \\n ', round((score_optimized[1]*100),  2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized model\n",
    "model_optimized.save('saved_models/model_optimized.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an image from the test set\n",
    "plt.imshow(X_test[1,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# Finding the index of the maximum value from the prediction list\n",
    "# and comparing the original value with the predicted value\n",
    "print(\"Original breed: \", labels['breed'][np.argmax(Y_test[1])])\n",
    "print(\"Predicted breed: \", labels['breed'][np.argmax(Y_pred_optimized[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this project, we explored the task of classifying dog breeds using convolutional neural networks (CNNs). We implemented two models: a baseline CNN model and an optimized CNN model with regularization techniques.\n",
    "\n",
    "# Results:\n",
    "The baseline model achieved an accuracy of approximately 33.9% on the test set, indicating moderate performance.\n",
    "The optimized model, despite incorporating regularization techniques, achieved a lower accuracy of around 18.64% on the test set. This unexpected result suggests that further experimentation and tuning of hyperparameters may be necessary to improve model performance.\n",
    "# Observations:\n",
    "Both models exhibited signs of overfitting, as evidenced by the significant difference in performance between the training and validation/test sets.\n",
    "The optimization techniques applied did not yield the expected improvements in model performance. This could be due to suboptimal hyperparameters or other factors influencing the training process.\n",
    "# Future Work:\n",
    "Further experimentation with hyperparameters, such as learning rate, batch size, and regularization strength, may be necessary to improve model performance.\n",
    "Exploring more advanced optimization techniques, such as dropout and batch normalization, could potentially yield better results.\n",
    "Conducting error analysis to identify the types of mistakes made by the models and potential avenues for improvement.\n",
    "In conclusion, while the implemented models provide a foundation for dog breed classification, there is room for improvement in terms of model performance and generalization to unseen data. Continued experimentation and refinement of the models are necessary to achieve better results."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c3458bb701476ea2acb41cd3819a608b09cbdda51706af7937ba7dfb7d47c62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
